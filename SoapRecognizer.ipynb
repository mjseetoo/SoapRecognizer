{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541e342b",
   "metadata": {},
   "source": [
    "This is a pretty cool project, it recognizes soap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4810ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this if you want to use your own training images\n",
    "#Otherwise, skip it.\n",
    "\n",
    "#To add your own images, put them in the \"train-images-unprocessed\"\n",
    "#make sure they are JPG, also make sure you go into the inputs.txt file \n",
    "#and make sure each image number corresponds to being soap or not soap.\n",
    "from PIL import Image\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "def resize_image(image_path, output_path, size=(400, 400)):\n",
    "    try:\n",
    "        img = Image.open(\"train-images-unprocessed/\"+image_path)\n",
    "        img = img.resize(size)\n",
    "        img.save(output_path)\n",
    "        print(\"Image resized successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error resizing image: {e}\")\n",
    "\n",
    "x = 0\n",
    "# get the path/directory\n",
    "folder_dir = \"train-images-unprocessed\"\n",
    "for images in os.listdir(folder_dir):\n",
    " \n",
    "    # check if the image ends with jpg\n",
    "    if (images.endswith(\".jpg\")):\n",
    "        output_image_path = \"train-images-processed/\" + str(x) + \".jpg\"\n",
    "        resize_image(images, output_image_path)\n",
    "    x+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2570417",
   "metadata": {},
   "source": [
    "Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfe3f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c9fb0",
   "metadata": {},
   "source": [
    "This cell defines the model and dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a86aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 50 * 50, 128)  # Adjusted input size to match the output of conv3\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Read labels from the text file\n",
    "        with open(os.path.join(\"labels.txt\"), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                image_name = parts[0]  # First part is the image name\n",
    "                \n",
    "                # Join remaining parts to form label (handling cases like \"not soap\")\n",
    "                label = \" \".join(parts[1:])\n",
    "                \n",
    "                # Append file extension to the image path\n",
    "                image_path = os.path.join(self.root_dir, f\"{image_name}.jpg\")\n",
    "                self.image_paths.append(image_path)\n",
    "                \n",
    "                # Map string labels to integers\n",
    "                if label == \"soap\":\n",
    "                    self.labels.append(0)\n",
    "                elif label == \"not_soap\":\n",
    "                    self.labels.append(1)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown label: {label}\")\n",
    "\n",
    "                # Print image paths and labels for debugging\n",
    "                print(f\"Image path: {self.image_paths[-1]}, Label: {self.labels[-1]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    \n",
    "# Specify device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify dataset directory, depending on how your computer handles unzips, you may need to replace this line with: \n",
    "#dataset_dir = \"output\"\n",
    "#This is a common for users with Macs\n",
    "dataset_dir = \"train-images-processed\"\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((400, 400)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = CustomDataset(root_dir=dataset_dir, transform=transform)\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create data loader\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define number of classes\n",
    "num_classes = 2 # assuming 2 classes: soap and not soap\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleCNN(num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79b140",
   "metadata": {},
   "source": [
    "This cell will train the model on the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23454b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(data_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:  # Print every 10 mini-batches\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{i + 1}/{len(data_loader)}], Loss: {running_loss / 10:.3f}\")\n",
    "            running_loss = 0.0\n",
    "            \n",
    "        epoch_loss = running_loss / len(data_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {epoch_loss:.3f}\")\n",
    "            \n",
    "torch.save(model.state_dict(), \"path_to_save_model.pth\")\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0820715",
   "metadata": {},
   "source": [
    "This cell will do the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b998b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "\n",
    "# Function to resize image\n",
    "def resize_image(input_image_path, output_image_path):\n",
    "    image = Image.open(input_image_path)\n",
    "    resized_image = image.resize((400, 400))\n",
    "    resized_image.save(output_image_path)\n",
    "    return output_image_path\n",
    "\n",
    "# Function to predict class of image\n",
    "def predict_image(image_path, model):\n",
    "    # Open the image and apply the same transformations used during training\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = ToTensor()\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        \n",
    "        # Map predicted label index to class name\n",
    "        class_names = [\"soap\", \"not soap\"]\n",
    "        predicted_class = class_names[predicted.item()]\n",
    "        \n",
    "        return predicted_class\n",
    "x = 0 \n",
    "while x <= 5:\n",
    "    #You may need to replace these next lines with \n",
    "    #input_image_path = \"testimages/test_image\"+str(x)+\".jpg\"\n",
    "    #output_image_path = \"testimages/resized_image\"+str(x)+\".jpg\"\n",
    "    #If you unzipped the files on a mac.\n",
    "    \n",
    "    input_image_path = \"test-images/test_image\"+str(x)+\".jpg\"\n",
    "    output_image_path = \"test-images/resized_image\"+str(x)+\".jpg\"\n",
    "\n",
    "    # Resize the input image\n",
    "    resized_image_path = resize_image(input_image_path, output_image_path)\n",
    "\n",
    "    # Load the trained model\n",
    "    model = SimpleCNN(num_classes=2)  # Assuming 2 classes: soap and not soap\n",
    "    model.load_state_dict(torch.load(\"path_to_save_model.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    # Predict the class of the resized image\n",
    "    predicted_class = predict_image(resized_image_path, model)\n",
    "    print(\"Is it soap?\", predicted_class)\n",
    "    x+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c8db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
